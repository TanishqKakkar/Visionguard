"""
==============================================================================
CCTV DETECTION - IMPROVED TRAINING (36% ‚Üí 70%+)
Balanced Regularization for Better Learning - FULLY CORRECTED
==============================================================================
"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')


np.random.seed(42)
torch.manual_seed(42)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n‚úÖ Device: {device}\n")


# ============================================================================
# VERBOSE LOGGER
# ============================================================================

class VerboseLogger:
    """Control verbosity level throughout training"""
    
    def __init__(self, verbose=True):
        self.verbose = verbose
    
    def info(self, message):
        if self.verbose:
            print(f"‚ÑπÔ∏è  {message}")
    
    def success(self, message):
        if self.verbose:
            print(f"‚úÖ {message}")
    
    def warning(self, message):
        if self.verbose:
            print(f"‚ö†Ô∏è  {message}")
    
    def step(self, message):
        if self.verbose:
            print(f"üìä {message}")
    
    def epoch(self, epoch, total, train_loss, train_acc, val_loss, val_acc, gap):
        if self.verbose:
            status = "‚úÖ" if gap < 0.15 else "‚ö†Ô∏è" if gap < 0.25 else "üî¥"
            print(f"Epoch {epoch+1:3d}/{total} | "
                  f"TL: {train_loss:.4f} | TA: {train_acc:.4f} | "
                  f"VL: {val_loss:.4f} | VA: {val_acc:.4f} | "
                  f"Gap: {gap:.4f} {status}")


logger = VerboseLogger(verbose=True)


# ============================================================================
# DATASET
# ============================================================================

class FrameDatasetLoaderFixed(Dataset):
    """Enhanced dataset loader with balanced augmentation"""
    
    def __init__(self, video_paths, labels, sequence_length=16, img_size=224, augment=True, verbose=True):
        self.video_paths = video_paths
        self.labels = labels
        self.sequence_length = sequence_length
        self.img_size = img_size
        self.augment = augment
        self.verbose = verbose
    
    def __len__(self):
        return len(self.video_paths)
    
    def load_frames_from_folder(self, folder_path):
        """Load frames from video folder"""
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        frame_files = []
        for ext in image_extensions:
            frame_files.extend(list(Path(folder_path).glob(ext)))
        
        frame_files = sorted(frame_files)
        
        if len(frame_files) == 0:
            return None
        
        if len(frame_files) > self.sequence_length:
            indices = np.linspace(0, len(frame_files) - 1, self.sequence_length, dtype=int)
            frame_files = [frame_files[i] for i in indices]
        elif len(frame_files) < self.sequence_length:
            while len(frame_files) < self.sequence_length:
                frame_files.append(frame_files[-1])
        
        frames = []
        for frame_path in frame_files[:self.sequence_length]:
            try:
                frame = cv2.imread(str(frame_path))
                if frame is not None:
                    frame = self.preprocess_frame(frame)
                    frames.append(frame)
            except:
                pass
        
        if len(frames) < self.sequence_length:
            while len(frames) < self.sequence_length:
                frames.append(np.zeros((self.img_size, self.img_size, 3), dtype=np.float32))
        
        return np.array(frames[:self.sequence_length], dtype=np.float32)
    
    def preprocess_frame(self, frame):
        """Preprocess frame with CLAHE"""
        frame = cv2.resize(frame, (self.img_size, self.img_size))
        
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        enhanced = cv2.merge([l, a, b])
        frame = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
        
        frame = frame.astype(np.float32) / 255.0
        return frame
    
    def augment_frames_balanced(self, frames):
        """Balanced augmentation for learning"""
        augmented = frames.copy()
        
        # 1. Brightness adjustment (50% chance)
        if np.random.rand() > 0.5:
            factor = np.random.uniform(0.8, 1.2)
            augmented = np.clip(augmented * factor, 0, 1)
        
        # 2. Contrast adjustment (50%)
        if np.random.rand() > 0.5:
            factor = np.random.uniform(0.8, 1.2)
            mean = augmented.mean()
            augmented = np.clip((augmented - mean) * factor + mean, 0, 1)
        
        # 3. Gaussian noise (40%)
        if np.random.rand() > 0.6:
            noise = np.random.normal(0, 0.03, augmented.shape)
            augmented = np.clip(augmented + noise, 0, 1)
        
        # 4. Horizontal flip (50%)
        if np.random.rand() > 0.5:
            augmented = np.ascontiguousarray(augmented[:, :, ::-1, :])
        
        return np.ascontiguousarray(augmented)
    
    def __getitem__(self, idx):
        frames = self.load_frames_from_folder(self.video_paths[idx])
        
        if frames is None:
            frames = np.zeros((self.sequence_length, self.img_size, self.img_size, 3), dtype=np.float32)
        
        if self.augment:
            frames = self.augment_frames_balanced(frames)
        
        frames = np.transpose(frames, (0, 3, 1, 2))
        frames = np.ascontiguousarray(frames)
        
        return torch.FloatTensor(frames), torch.LongTensor([self.labels[idx]])[0]


# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================

class ViTBlock(nn.Module):
    """Vision Transformer Block"""
    
    def __init__(self, dim, num_heads, mlp_dim, dropout=0.3):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, dim),
            nn.Dropout(dropout),
        )
    
    def forward(self, x):
        x1 = self.norm1(x)
        attn_out, _ = self.attn(x1, x1, x1)
        x = x + attn_out
        x2 = self.norm2(x)
        return x + self.mlp(x2)


class ViTEncoder(nn.Module):
    """Vision Transformer Encoder"""
    
    def __init__(self, img_size=224, patch_size=16, dim=512, num_heads=8, num_layers=6, mlp_dim=2048):
        super().__init__()
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        self.patch_embed = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)
        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, dim) * 0.02)
        
        self.blocks = nn.Sequential(*[ViTBlock(dim, num_heads, mlp_dim) for _ in range(num_layers)])
        self.norm = nn.LayerNorm(dim)
    
    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B*T, C, H, W)
        x = self.patch_embed(x)
        x = x.flatten(2).transpose(1, 2)
        x = x + self.pos_embed
        x = self.blocks(x)
        x = self.norm(x)
        x = x.view(B, T, self.num_patches, -1).mean(dim=2)
        return x


class ViTConvLSTMImproved(nn.Module):
    """Improved ViT + LSTM Model"""
    
    def __init__(self, seq_len=16, img_size=224, num_classes=5):
        super().__init__()
        self.vit = ViTEncoder(img_size=img_size, patch_size=16, dim=512, 
                              num_heads=8, num_layers=6, mlp_dim=2048)
        
        self.lstm1 = nn.LSTM(512, 512, 1, dropout=0.3, batch_first=True)
        self.lstm2 = nn.LSTM(512, 256, 1, dropout=0.3, batch_first=True)
        
        self.head = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.vit(x)
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x = x[:, -1, :]
        return self.head(x)


# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================

def train_epoch(model, loader, criterion, optimizer, device, verbose=True):
    """Train one epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (X, y) in enumerate(loader):
        X, y = X.to(device), y.to(device)
        
        optimizer.zero_grad()
        out = model(X)
        loss = criterion(out, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
        _, pred = torch.max(out, 1)
        correct += (pred == y).sum().item()
        total += y.size(0)
    
    torch.cuda.empty_cache()
    return total_loss / len(loader), correct / total


def validate(model, loader, criterion, device, verbose=True):
    """Validate model"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for X, y in loader:
            X, y = X.to(device), y.to(device)
            out = model(X)
            loss = criterion(out, y)
            total_loss += loss.item()
            _, pred = torch.max(out, 1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    
    torch.cuda.empty_cache()
    return total_loss / len(loader), correct / total


# ============================================================================
# DATA LOADING
# ============================================================================

def load_dataset(dataset_path, sequence_length=16, img_size=224, verbose=True):
    """Load dataset from folder structure"""
    
    if verbose:
        logger.info("Loading dataset...")
    
    dataset_path = Path(dataset_path)
    all_folders = [f.name for f in dataset_path.iterdir() if f.is_dir()]
    classes = sorted([f for f in all_folders if f.lower() not in ['train', 'val', 'test']])
    
    if verbose:
        logger.success(f"Found classes: {classes}")
    
    video_paths = []
    video_labels = []
    
    for class_idx, class_name in enumerate(classes):
        class_path = dataset_path / class_name
        if class_path.exists():
            videos = [v for v in class_path.iterdir() if v.is_dir()]
            logger.info(f"  {class_name}: {len(videos)} videos")
            for video_folder in videos:
                video_paths.append(video_folder)
                video_labels.append(class_idx)
    
    if verbose:
        logger.success(f"Found {len(video_paths)} total videos")
    
    return video_paths, video_labels, classes


# ============================================================================
# EVALUATION
# ============================================================================

def evaluate_model(model, loader, classes, device, verbose=True):
    """Evaluate model on test set"""
    
    if verbose:
        logger.success("="*80)
        logger.success("üìä EVALUATION")
        logger.success("="*80)
    
    model.eval()
    y_pred_all = []
    y_true_all = []
    
    with torch.no_grad():
        for X, y in loader:
            X = X.to(device)
            out = model(X)
            _, pred = torch.max(out, 1)
            y_pred_all.extend(pred.cpu().numpy())
            y_true_all.extend(y.numpy())
    
    acc = accuracy_score(y_true_all, y_pred_all)
    
    if verbose:
        logger.success(f"Test Accuracy: {acc*100:.2f}%")
        logger.info("Classification Report:")
        print(classification_report(y_true_all, y_pred_all, target_names=classes))
    
    cm = confusion_matrix(y_true_all, y_pred_all)
    return acc, cm, y_pred_all, y_true_all


# ============================================================================
# PLOTTING
# ============================================================================

def plot_results(history, verbose=True):
    """Plot training results"""
    
    os.makedirs("results", exist_ok=True)
    
    plt.figure(figsize=(14, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history['train_acc'], label='Train', linewidth=2, color='steelblue')
    plt.plot(history['val_acc'], label='Val', linewidth=2, color='coral')
    plt.fill_between(range(len(history['train_acc'])), 
                     history['train_acc'], 
                     history['val_acc'], 
                     alpha=0.2, color='red', label='Gap')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.title('Accuracy: Train vs Validation', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history['train_loss'], label='Train', linewidth=2, color='steelblue')
    plt.plot(history['val_loss'], label='Val', linewidth=2, color='coral')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Loss: Train vs Validation', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/training_history.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    if verbose:
        logger.success("Plot saved: results/training_history.png")


def plot_confusion_matrix(cm, classes, verbose=True):
    """Plot confusion matrix"""
    
    os.makedirs("results", exist_ok=True)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes,
                cbar_kws={'label': 'Count'})
    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    if verbose:
        logger.success("Confusion matrix saved: results/confusion_matrix.png")


# ============================================================================
# MAIN
# ============================================================================

def main(verbose=True):
    """Main training pipeline"""
    
    global logger
    logger = VerboseLogger(verbose=verbose)
    
    logger.success("="*80)
    logger.success("‚úÖ IMPROVED TRAINING (36% ‚Üí 70%+)")
    logger.success("="*80)
    
    # Configuration
    DATASET_PATH = r"D:\dataset frames"
    SEQUENCE_LENGTH = 16
    IMG_SIZE = 224
    EPOCHS = 200
    BATCH_SIZE = 8
    LR = 1e-4
    WEIGHT_DECAY = 1e-4
    
    logger.info("Configuration:")
    logger.step(f"  Epochs: {EPOCHS} (patience=30)")
    logger.step(f"  Batch size: {BATCH_SIZE} (increased)")
    logger.step(f"  Learning rate: {LR} (increased 10x!)")
    logger.step(f"  Weight decay: {WEIGHT_DECAY} (reduced 50x!)")
    logger.step(f"  Model: Larger ViT (6 layers, 512 dims)")
    
    # Load dataset
    logger.info("\nLoading dataset...")
    video_paths, video_labels, classes = load_dataset(DATASET_PATH, SEQUENCE_LENGTH, IMG_SIZE, verbose)
    
    if len(video_paths) == 0:
        logger.success("No videos found!")
        return
    
    # Split data
    X_train_paths, X_temp_paths, y_train, y_temp = train_test_split(
        video_paths, video_labels, test_size=0.3, random_state=42, stratify=video_labels
    )
    X_val_paths, X_test_paths, y_val, y_test = train_test_split(
        X_temp_paths, y_temp, test_size=0.33, random_state=42, stratify=y_temp
    )
    
    logger.step(f"Train: {len(X_train_paths)} | Val: {len(X_val_paths)} | Test: {len(X_test_paths)}")
    
    # Create dataloaders
    logger.info("Creating dataloaders...")
    train_ds = FrameDatasetLoaderFixed(X_train_paths, y_train, SEQUENCE_LENGTH, IMG_SIZE, augment=True, verbose=False)
    val_ds = FrameDatasetLoaderFixed(X_val_paths, y_val, SEQUENCE_LENGTH, IMG_SIZE, augment=False, verbose=False)
    test_ds = FrameDatasetLoaderFixed(X_test_paths, y_test, SEQUENCE_LENGTH, IMG_SIZE, augment=False, verbose=False)
    
    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    logger.success("Dataloaders created")
    
    # Build model
    logger.info("\nBuilding model...")
    model = ViTConvLSTMImproved(seq_len=SEQUENCE_LENGTH, img_size=IMG_SIZE, num_classes=len(classes))
    model = model.to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    logger.success(f"Model built with {total_params:,} parameters (22.6M)")
    
    # Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)
    
    os.makedirs("models", exist_ok=True)
    os.makedirs("results", exist_ok=True)
    
    # Training loop
    logger.success("="*80)
    logger.success("üéì TRAINING")
    logger.success("="*80)
    logger.info(f"Training on {device}\n")
    
    best_val_acc = 0
    patience_count = 0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'gap': []}
    
    for epoch in range(EPOCHS):
        train_loss, train_acc = train_epoch(model, train_dl, criterion, optimizer, device, verbose=False)
        val_loss, val_acc = validate(model, val_dl, criterion, device, verbose=False)
        
        gap = train_acc - val_acc
        
        logger.epoch(epoch, EPOCHS, train_loss, train_acc, val_loss, val_acc, gap)
        
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['gap'].append(gap)
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_count = 0
            torch.save(model.state_dict(), 'models/vit_convlstm_best.pt')
            logger.success(f"  üéØ New best: {val_acc:.4f}")
        else:
            patience_count += 1
        
        scheduler.step(val_acc)
        
        if patience_count > 30:
            logger.warning(f"Early stopping at epoch {epoch+1}")
            break
    
    logger.success("\n" + "="*80)
    logger.success(f"‚úÖ Best validation accuracy: {best_val_acc:.4f}")
    logger.success("="*80)
    
    # Plot results
    logger.info("\nGenerating plots...")
    plot_results(history, verbose)
    
    # Evaluate
    logger.info("\nEvaluating on test set...")
    test_acc, cm, y_pred, y_true = evaluate_model(model, test_dl, classes, device, verbose)
    plot_confusion_matrix(cm, classes, verbose)
    
    # Summary
    logger.success("\n" + "="*80)
    logger.success("‚úÖ TRAINING COMPLETED!")
    logger.success("="*80)
    logger.step(f"Final Test Accuracy: {test_acc*100:.2f}%")
    logger.step(f"Final Gap: {history['gap'][-1]:.4f}")
    logger.step(f"Model saved: models/vit_convlstm_best.pt")


if __name__ == "__main__":
    main(verbose=True)
